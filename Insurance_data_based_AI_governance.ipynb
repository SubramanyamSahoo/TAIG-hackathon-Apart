{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9ncmrvZTkwP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas matplotlib seaborn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDjJ7YBHTuSj",
        "outputId": "5b9ad0c0-862a-4173-facf-61133508dca5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional but helpful\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    _HAS_SEABORN = True\n",
        "except Exception:\n",
        "    _HAS_SEABORN = False\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "def pareto_rvs(rng, s_min, xi, size):\n",
        "    # Tail form: P(S > s) = (s_min / s)^xi for s >= s_min\n",
        "    u = rng.random(size=size)\n",
        "    return s_min * (u ** (-1.0 / xi))\n",
        "\n",
        "\n",
        "def cvar(x, q=0.99):\n",
        "    x = np.asarray(x)\n",
        "    threshold = np.quantile(x, q)\n",
        "    tail = x[x >= threshold]\n",
        "    if len(tail) == 0:\n",
        "        return float(threshold)\n",
        "    return float(tail.mean())\n",
        "\n",
        "\n",
        "def bootstrap_iqr_ratio(rng, x, B=400):\n",
        "    # IQR divided by median for a statistic, here mean of sampled years\n",
        "    x = np.asarray(x)\n",
        "    n = len(x)\n",
        "    stats = np.empty(B)\n",
        "    for b in range(B):\n",
        "        idx = rng.integers(0, n, size=n)\n",
        "        stats[b] = float(np.mean(x[idx]))\n",
        "    q25, q75 = np.quantile(stats, [0.25, 0.75])\n",
        "    med = np.median(stats)\n",
        "    return float((q75 - q25) / (med + 1e-12))\n",
        "\n",
        "\n",
        "def simulate_losses(\n",
        "    rng,\n",
        "    N=200,\n",
        "    Y=5000,\n",
        "    mu_e=0.0,\n",
        "    sigma_e=1.0,\n",
        "    sigma_u=0.3,\n",
        "    alpha=-2.5,\n",
        "    beta_e=1.0,\n",
        "    beta_c=-0.6,\n",
        "    s_min=1.0,\n",
        "    xi=1.3,\n",
        "    p=0.05,\n",
        "    delta=1.0,\n",
        "    dxi=0.3,\n",
        "    gamma0=-2.0,\n",
        "    gammas=0.8,\n",
        "    gammac=0.3,\n",
        "    regime=\"C\",\n",
        "    backstop_share=0.0\n",
        "):\n",
        "    e = rng.lognormal(mean=mu_e, sigma=sigma_e, size=N)\n",
        "    c = rng.normal(loc=0.0, scale=1.0, size=N)\n",
        "    u = rng.normal(loc=0.0, scale=sigma_u, size=N)\n",
        "\n",
        "    loglam = alpha + beta_e * np.log(e) + beta_c * c + u\n",
        "\n",
        "    # True annual losses per organization\n",
        "    L_true = np.zeros((N, Y), dtype=float)\n",
        "\n",
        "    # Observed annual loss proxy per organization under reporting regime\n",
        "    # This is not used as true loss, but it affects the estimation precision proxy\n",
        "    L_obs = np.zeros((N, Y), dtype=float)\n",
        "\n",
        "    Zs = rng.binomial(1, p, size=Y)\n",
        "\n",
        "    # Regime controls visibility via intercept shift\n",
        "    if regime == \"A\":\n",
        "        g0_shift = -2.0\n",
        "    elif regime == \"B\":\n",
        "        g0_shift = 1.0\n",
        "    elif regime == \"C\":\n",
        "        g0_shift = 1.0\n",
        "    else:\n",
        "        raise ValueError(\"regime must be A, B, or C\")\n",
        "\n",
        "    for t in range(Y):\n",
        "        Z = int(Zs[t])\n",
        "        lam_t = np.exp(loglam + Z * delta)\n",
        "        K = rng.poisson(lam_t)\n",
        "\n",
        "        xi_t = xi + Z * dxi\n",
        "\n",
        "        for i in range(N):\n",
        "            k = int(K[i])\n",
        "            if k == 0:\n",
        "                continue\n",
        "\n",
        "            S = pareto_rvs(rng, s_min=s_min, xi=xi_t, size=k)\n",
        "            loss_true = float(np.sum(S))\n",
        "\n",
        "            # Apply systemic facility backstop only in shock state\n",
        "            if Z == 1 and backstop_share > 0:\n",
        "                loss_true = (1.0 - backstop_share) * loss_true\n",
        "\n",
        "            L_true[i, t] = loss_true\n",
        "\n",
        "            # Reporting selection to define what is observed\n",
        "            report_p = sigmoid((gamma0 + g0_shift) + gammas * np.log(S) + gammac * c[i])\n",
        "            R = rng.binomial(1, report_p, size=k)\n",
        "            if np.any(R == 1):\n",
        "                L_obs[i, t] = float(np.sum(S[R == 1]))\n",
        "            else:\n",
        "                L_obs[i, t] = 0.0\n",
        "\n",
        "    return e, c, L_true, L_obs\n",
        "\n",
        "\n",
        "def compute_metrics(rng, e, c, L_true, L_obs, q=0.99):\n",
        "    N = L_true.shape[0]\n",
        "    M = np.zeros(N, dtype=float)\n",
        "    prem_unc = np.zeros(N, dtype=float)\n",
        "\n",
        "    # Use L_obs to model estimation uncertainty (proxy for data availability)\n",
        "    # Use L_true to compute solvency relevant tail multiple\n",
        "    for i in range(N):\n",
        "        li_true = L_true[i]\n",
        "        mu_true = float(np.mean(li_true))\n",
        "        tail_true = cvar(li_true, q=q)\n",
        "        M[i] = float(tail_true / (mu_true + 1e-12))\n",
        "\n",
        "        li_obs = L_obs[i]\n",
        "        prem_unc[i] = bootstrap_iqr_ratio(rng, li_obs, B=300)\n",
        "\n",
        "    premium_proxy = np.mean(L_true, axis=1)\n",
        "\n",
        "    lo = float(np.mean(premium_proxy[c <= np.quantile(c, 0.25)]))\n",
        "    hi = float(np.mean(premium_proxy[c >= np.quantile(c, 0.75)]))\n",
        "    discount = float(1.0 - (hi / (lo + 1e-12)))\n",
        "\n",
        "    out = {\n",
        "        \"median_premium_uncertainty_ratio\": float(np.median(prem_unc)),\n",
        "        \"median_capital_load_proxy_multiple\": float(np.median(M)),\n",
        "        \"premium_discount_strong_controls\": float(discount),\n",
        "        \"exclusion_rate_T6\": float(np.mean(M > 6.0)),\n",
        "        \"M\": M,\n",
        "        \"prem_unc\": prem_unc,\n",
        "        \"premium_proxy\": premium_proxy\n",
        "    }\n",
        "    return out\n",
        "\n",
        "\n",
        "def make_tables_for_section_7(seed=0, out_dir=\"outputs\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Table 1 style comparison across regimes\n",
        "    rows = []\n",
        "    for regime in [\"A\", \"B\", \"C\"]:\n",
        "        e, c, L_true, L_obs = simulate_losses(\n",
        "            rng=np.random.default_rng(seed + 10),\n",
        "            regime=regime,\n",
        "            p=0.05,\n",
        "            xi=1.3,\n",
        "            backstop_share=0.0\n",
        "        )\n",
        "        metrics = compute_metrics(np.random.default_rng(seed + 20), e, c, L_true, L_obs, q=0.99)\n",
        "        rows.append({\n",
        "            \"Data regime\": regime,\n",
        "            \"Premium uncertainty ratio\": metrics[\"median_premium_uncertainty_ratio\"],\n",
        "            \"Capital load proxy multiple\": metrics[\"median_capital_load_proxy_multiple\"],\n",
        "            \"Premium discount from strong controls\": metrics[\"premium_discount_strong_controls\"]\n",
        "        })\n",
        "\n",
        "    df1 = pd.DataFrame(rows)\n",
        "    df1.to_csv(os.path.join(out_dir, \"table_1.csv\"), index=False)\n",
        "    with open(os.path.join(out_dir, \"table_1.tex\"), \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(df1.to_latex(index=False, float_format=lambda x: f\"{x:.3f}\"))\n",
        "\n",
        "    # Table 2 style backstop variation under regime C\n",
        "    rows2 = []\n",
        "    for b in [0.0, 0.3, 0.6]:\n",
        "        e, c, L_true, L_obs = simulate_losses(\n",
        "            rng=np.random.default_rng(seed + 30),\n",
        "            regime=\"C\",\n",
        "            p=0.05,\n",
        "            xi=1.3,\n",
        "            backstop_share=b\n",
        "        )\n",
        "        metrics = compute_metrics(np.random.default_rng(seed + 40), e, c, L_true, L_obs, q=0.99)\n",
        "        rows2.append({\n",
        "            \"Backstop share\": b,\n",
        "            \"Capital load proxy multiple\": metrics[\"median_capital_load_proxy_multiple\"],\n",
        "            \"Exclusion rate T equals 6\": metrics[\"exclusion_rate_T6\"]\n",
        "        })\n",
        "\n",
        "    df2 = pd.DataFrame(rows2)\n",
        "    df2.to_csv(os.path.join(out_dir, \"table_2.csv\"), index=False)\n",
        "    with open(os.path.join(out_dir, \"table_2.tex\"), \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(df2.to_latex(index=False, float_format=lambda x: f\"{x:.3f}\"))\n",
        "\n",
        "    return df1, df2\n",
        "\n",
        "\n",
        "def stress_test_grid(seed=0, out_dir=\"outputs\", regime=\"C\", backstop_share=0.0, T=6.0):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    ps = [0.0, 0.01, 0.02, 0.05, 0.10]\n",
        "    xis = [1.1, 1.3, 1.6, 2.0]\n",
        "\n",
        "    records = []\n",
        "    for p in ps:\n",
        "        for xi in xis:\n",
        "            e, c, L_true, L_obs = simulate_losses(\n",
        "                rng=np.random.default_rng(seed + int(1000 * p) + int(100 * xi)),\n",
        "                regime=regime,\n",
        "                p=p,\n",
        "                xi=xi,\n",
        "                backstop_share=backstop_share\n",
        "            )\n",
        "            metrics = compute_metrics(np.random.default_rng(seed + 999), e, c, L_true, L_obs, q=0.99)\n",
        "            M = metrics[\"M\"]\n",
        "            records.append({\n",
        "                \"p\": p,\n",
        "                \"xi\": xi,\n",
        "                \"median_M\": float(np.median(M)),\n",
        "                \"exclusion_rate\": float(np.mean(M > T)),\n",
        "                \"median_premium_unc\": metrics[\"median_premium_uncertainty_ratio\"]\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(os.path.join(out_dir, f\"stress_grid_regime_{regime}.csv\"), index=False)\n",
        "    return df\n",
        "\n",
        "\n",
        "def plot_heatmap(df, value_col, title, out_path):\n",
        "    pivot = df.pivot(index=\"xi\", columns=\"p\", values=value_col)\n",
        "    plt.figure(figsize=(8, 4.8))\n",
        "    if _HAS_SEABORN:\n",
        "        ax = sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
        "    else:\n",
        "        ax = plt.gca()\n",
        "        im = ax.imshow(pivot.values, aspect=\"auto\")\n",
        "        ax.set_xticks(range(len(pivot.columns)))\n",
        "        ax.set_xticklabels([str(x) for x in pivot.columns])\n",
        "        ax.set_yticks(range(len(pivot.index)))\n",
        "        ax.set_yticklabels([str(x) for x in pivot.index])\n",
        "        plt.colorbar(im, ax=ax)\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Systemic shock probability p\")\n",
        "    ax.set_ylabel(\"Tail index xi\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=220)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_exclusion_boundary_curve(seed=0, out_dir=\"outputs\", regime=\"C\", xi=1.3, backstop_share=0.0, T=6.0):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    ps = np.linspace(0.0, 0.12, 13)\n",
        "    rates = []\n",
        "    medMs = []\n",
        "\n",
        "    for p in ps:\n",
        "        e, c, L_true, L_obs = simulate_losses(\n",
        "            rng=np.random.default_rng(seed + int(10000 * p)),\n",
        "            regime=regime,\n",
        "            p=float(p),\n",
        "            xi=float(xi),\n",
        "            backstop_share=backstop_share\n",
        "        )\n",
        "        metrics = compute_metrics(np.random.default_rng(seed + 777), e, c, L_true, L_obs, q=0.99)\n",
        "        rates.append(float(np.mean(metrics[\"M\"] > T)))\n",
        "        medMs.append(float(np.median(metrics[\"M\"])))\n",
        "\n",
        "    plt.figure(figsize=(7.5, 4.5))\n",
        "    plt.plot(ps, rates, marker=\"o\", label=\"Exclusion rate\")\n",
        "    plt.axhline(0.5, color=\"gray\", linewidth=1.0)\n",
        "    plt.title(\"Exclusion rate as a function of shock probability\")\n",
        "    plt.xlabel(\"Systemic shock probability p\")\n",
        "    plt.ylabel(\"Fraction above exclusion threshold\")\n",
        "    plt.ylim(-0.02, 1.02)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_dir, \"fig_exclusion_boundary_curve.png\"), dpi=220)\n",
        "    plt.close()\n",
        "\n",
        "    out = pd.DataFrame({\"p\": ps, \"exclusion_rate\": rates, \"median_M\": medMs})\n",
        "    out.to_csv(os.path.join(out_dir, \"exclusion_boundary_curve.csv\"), index=False)\n",
        "    return out\n",
        "\n",
        "\n",
        "def write_appendix_equations_tex(out_dir=\"outputs\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    tex = r\"\"\"\n",
        "\\section{Appendix A: Stochastic loss model and reporting regimes}\n",
        "\n",
        "This appendix states the full simulator specification used for the stress tests in Section 7.\n",
        "\n",
        "\\subsection{Organization variables}\n",
        "For organization $i \\in \\{1,\\ldots,N\\}$, exposure and control are\n",
        "\n",
        "$$\n",
        "\n",
        "e_i \\sim \\mathrm{LogNormal}(\\mu_e,\\sigma_e), \\qquad c_i \\sim \\mathcal{N}(0,1),\n",
        "\n",
        "$$\n",
        "and an organization random effect may be included\n",
        "\n",
        "$$\n",
        "\n",
        "u_i \\sim \\mathcal{N}(0,\\sigma_u).\n",
        "\n",
        "$$\n",
        "\n",
        "\\subsection{Incident frequency}\n",
        "Annual incident count is\n",
        "\n",
        "$$\n",
        "\n",
        "K_i \\sim \\mathrm{Poisson}(\\lambda_i), \\qquad \\log \\lambda_i = \\alpha + \\beta_e \\log e_i + \\beta_c c_i + u_i.\n",
        "\n",
        "$$\n",
        "\n",
        "\\subsection{Systemic shock correlation}\n",
        "A systemic shock indicator is drawn each year:\n",
        "\n",
        "$$\n",
        "\n",
        "Z \\sim \\mathrm{Bernoulli}(p).\n",
        "\n",
        "$$\n",
        "In shock years, frequency increases by $\\delta$:\n",
        "\n",
        "$$\n",
        "\n",
        "\\log \\lambda_i \\leftarrow \\log \\lambda_i + Z\\delta.\n",
        "\n",
        "$$\n",
        "\n",
        "\\subsection{Severity}\n",
        "Each incident has heavy tailed severity:\n",
        "\n",
        "$$\n",
        "\n",
        "S_{ij} \\sim \\mathrm{Pareto}(s_{\\min}, \\xi + Z \\Delta_{\\xi}).\n",
        "\n",
        "$$\n",
        "Total annual loss is\n",
        "\n",
        "$$\n",
        "\n",
        "L_i = \\sum_{j=1}^{K_i} S_{ij}.\n",
        "\n",
        "$$\n",
        "\n",
        "\\subsection{Reporting and selection}\n",
        "Visibility of an incident is\n",
        "\n",
        "$$\n",
        "\n",
        "R_{ij} \\sim \\mathrm{Bernoulli}(\\pi_{ij}),\n",
        "\\qquad\n",
        "\\pi_{ij} = \\sigma(\\gamma_0 + \\gamma_s \\log S_{ij} + \\gamma_c c_i),\n",
        "\n",
        "$$\n",
        "where $\\sigma(\\cdot)$ is the logistic sigmoid. Reporting regimes modify the effective intercept to model public only versus confidential reporting.\n",
        "\n",
        "\\subsection{Systemic facility backstop}\n",
        "When the systemic facility pays share $b$ of losses in shock years, net loss becomes\n",
        "\n",
        "$$\n",
        "\n",
        "L_i^{\\mathrm{net}} =\n",
        "\\begin{cases}\n",
        "(1-b)L_i & \\text{if } Z=1,\\\\\n",
        "L_i & \\text{if } Z=0.\n",
        "\\end{cases}\n",
        "\n",
        "$$\n",
        "\n",
        "\\subsection{Capital load proxy multiple}\n",
        "Let $q$ be a high percentile. Define conditional value at risk:\n",
        "\n",
        "$$\n",
        "\n",
        "\\mathrm{CVaR}_q(L_i) = \\mathbb{E}[L_i \\mid L_i \\ge \\mathrm{VaR}_q(L_i)].\n",
        "\n",
        "$$\n",
        "Define the capital load proxy multiple:\n",
        "\n",
        "$$\n",
        "\n",
        "M_i = \\frac{\\mathrm{CVaR}_q(L_i)}{\\mathbb{E}[L_i]}.\n",
        "\n",
        "$$\n",
        "\n",
        "\\subsection{Exclusion boundary}\n",
        "Given threshold $T$, insurance collapse into exclusions is operationalized as\n",
        "\n",
        "$$\n",
        "\n",
        "M_i > T.\n",
        "\n",
        "$$\n",
        "The exclusion rate is the fraction of organizations with $M_i > T$.\n",
        "\"\"\"\n",
        "    path = os.path.join(out_dir, \"appendix_equations.tex\")\n",
        "    with open(path, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(tex.strip() + \"\\n\")\n",
        "    return path\n",
        "\n",
        "\n",
        "def write_mermaid_files(out_dir=\"outputs\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    mermaids = {}\n",
        "\n",
        "    mermaids[\"mermaid_fig_1_system_overview.mmd\"] = r\"\"\"\n",
        "mindmap\n",
        "  root((Insurance mediated inclusion for frontier AI governance))\n",
        "    Core objective\n",
        "      Include non state actors\n",
        "      Incentivise safer development and deployment\n",
        "      Support international agreement stability\n",
        "    Data layer\n",
        "      Exposure data\n",
        "        Deployment channels\n",
        "        Tool access\n",
        "        Update cadence\n",
        "        Usage volume bins\n",
        "      Control data\n",
        "        Evaluations\n",
        "        Monitoring and logging\n",
        "        Access restrictions\n",
        "        Incident response capability\n",
        "      Incident data\n",
        "        Hazard class\n",
        "        Severity\n",
        "        Discovery channel\n",
        "        Remediation action\n",
        "      Data governance\n",
        "        Confidential reporting\n",
        "        Aggregation releases\n",
        "        Audit triggers\n",
        "    Insurance and liability\n",
        "      Underwriting incentives\n",
        "        Premium gradients\n",
        "        Control requirements\n",
        "      Liability shaping\n",
        "        Negligence standards\n",
        "        Safe harbor for reporting\n",
        "      Systemic risk handling\n",
        "        Pooling facility\n",
        "        Reinsurance layer\n",
        "    International agreement hooks\n",
        "      Qualified coverage requirement\n",
        "      Mutual recognition\n",
        "      Procurement linkage\n",
        "      Supply chain leverage\n",
        "\"\"\".strip()\n",
        "\n",
        "    mermaids[\"mermaid_fig_2_inclusion_mechanism.mmd\"] = r\"\"\"\n",
        "flowchart TB\n",
        "  A[International agreement commitments] --> B[Domestic implementing rules]\n",
        "  B --> C[Qualified coverage requirement for frontier scale training or deployment]\n",
        "  C --> D[Non state actors seek coverage]\n",
        "  D --> E[Insurers require reporting and controls]\n",
        "  E --> F[Standardised risk data submission]\n",
        "  E --> G[Baseline control adoption]\n",
        "  C --> H[Market access conditions]\n",
        "  H --> I[Public procurement eligibility]\n",
        "  H --> J[Regulated sector contracting]\n",
        "  H --> K[Cross border cloud contracting norms]\n",
        "  F --> L[Confidential data trust or insurer consortium]\n",
        "  L --> M[Aggregate hazard statistics]\n",
        "  L --> N[Audit triggers and dispute resolution]\n",
        "  M --> O[Pricing improves and premiums reward controls]\n",
        "  N --> P[Enforcement through coverage suspension]\n",
        "\"\"\".strip()\n",
        "\n",
        "    mermaids[\"mermaid_fig_3_data_flow_confidentiality.mmd\"] = r\"\"\"\n",
        "flowchart LR\n",
        "  A[Organization logs and internal reports] --> B[Local summarisation and validation]\n",
        "  B --> C[Confidential submission package]\n",
        "  C --> D[Insurer underwriting view]\n",
        "  C --> E[Regulator or treaty auditor view]\n",
        "  C --> F[Public aggregate release]\n",
        "  D --> G[Premium setting and control requirements]\n",
        "  E --> H[Compliance checks and targeted audits]\n",
        "  F --> I[Hazard trend monitoring]\n",
        "  J[Safe harbor legal protection] -.-> C\n",
        "  K[Contractual penalties for misreporting] -.-> C\n",
        "\"\"\".strip()\n",
        "\n",
        "    mermaids[\"mermaid_fig_4_liability_insurance_interaction.mmd\"] = r\"\"\"\n",
        "flowchart TD\n",
        "  A[Incident occurs] --> B{Organization reports promptly}\n",
        "  B -->|Yes| C[Safe harbor pathway]\n",
        "  B -->|No| D[Adverse inference pathway]\n",
        "  C --> E[Reduced punitive damages]\n",
        "  C --> F[Lower premium increase]\n",
        "  C --> G[Eligibility for systemic facility support]\n",
        "  D --> H[Higher expected damages]\n",
        "  D --> I[Higher premiums or non renewal]\n",
        "  D --> J[Regulatory escalation]\n",
        "  K[Baseline controls present] --> C\n",
        "  K --> D\n",
        "\"\"\".strip()\n",
        "\n",
        "    mermaids[\"mermaid_fig_5_stress_workflow.mmd\"] = r\"\"\"\n",
        "flowchart TD\n",
        "  A[Choose stress parameters p and xi] --> B[Run synthetic loss simulator]\n",
        "  B --> C[Compute capital load proxy distribution M]\n",
        "  C --> D[Select exclusion threshold T]\n",
        "  D --> E{Is M greater than T}\n",
        "  E -->|Yes| F[Insurance retreats into exclusions]\n",
        "  E -->|No| G[Insurance remains viable]\n",
        "  F --> H[Governance response]\n",
        "  H --> I[Strengthen reporting and audit requirements]\n",
        "  H --> J[Add systemic risk facility or reinsurance layer]\n",
        "  G --> K[Governance response]\n",
        "  K --> L[Require qualified coverage in agreement]\n",
        "  K --> M[Condition premiums on baseline controls]\n",
        "  K --> N[Use safe harbor to reward reporting]\n",
        "\"\"\".strip()\n",
        "\n",
        "    mermaids[\"mermaid_fig_6_systemic_facility_design.mmd\"] = r\"\"\"\n",
        "flowchart TB\n",
        "  A[Systemic facility trigger definition] --> B[Event classification and verification]\n",
        "  B --> C[Facility pays share of systemic losses]\n",
        "  C --> D[Insurers maintain broad coverage]\n",
        "  C --> E[Premiums remain stable]\n",
        "  F[Eligibility conditions] --> G[Participation in reporting standard]\n",
        "  F --> H[Baseline control compliance]\n",
        "  F --> I[Audit cooperation]\n",
        "  G --> C\n",
        "  H --> C\n",
        "  I --> C\n",
        "  J[Moral hazard controls] --> K[Deductibles and co insurance]\n",
        "  J --> L[Penalties for misreporting]\n",
        "  J --> M[Rate adjustments after repeated negligence]\n",
        "\"\"\".strip()\n",
        "\n",
        "    mermaids[\"mermaid_fig_7_mutual_recognition_map.mmd\"] = r\"\"\"\n",
        "flowchart LR\n",
        "  A[Signatory state A rules] --> B[Qualified insurer criteria]\n",
        "  A --> C[Qualified auditor criteria]\n",
        "  A --> D[Liability safe harbor statute]\n",
        "  E[Signatory state B rules] --> B\n",
        "  E --> C\n",
        "  E --> D\n",
        "  B --> F[Mutual recognition list]\n",
        "  C --> F\n",
        "  F --> G[Cross border coverage portability]\n",
        "  F --> H[Cross border audit acceptance]\n",
        "  G --> I[Non state actors comply to access markets]\n",
        "  H --> I\n",
        "\"\"\".strip()\n",
        "\n",
        "    mermaids[\"mermaid_fig_8_control_incentive_gradient.mmd\"] = r\"\"\"\n",
        "flowchart TD\n",
        "  A[Standardised exposure and control data] --> B[Underwriting risk differentiation]\n",
        "  B --> C[Premium gradients]\n",
        "  C --> D[Adoption of baseline controls]\n",
        "  D --> E[Lower incident frequency and severity]\n",
        "  E --> F[Lower capital load and broader coverage]\n",
        "  F --> C\n",
        "\"\"\".strip()\n",
        "\n",
        "    for name, text in mermaids.items():\n",
        "        with open(os.path.join(out_dir, name), \"w\", encoding=\"utf8\") as f:\n",
        "            f.write(text + \"\\n\")\n",
        "\n",
        "    return list(mermaids.keys())\n",
        "\n",
        "\n",
        "def main():\n",
        "    out_dir = \"outputs\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Tables for Section 7 discussion, but you can include them in appendix if you prefer\n",
        "    df1, df2 = make_tables_for_section_7(seed=0, out_dir=out_dir)\n",
        "\n",
        "    # Stress grid for regime C and figures for main Section 7\n",
        "    df_grid = stress_test_grid(seed=0, out_dir=out_dir, regime=\"C\", backstop_share=0.0, T=6.0)\n",
        "\n",
        "    # Figures for Section 7\n",
        "    plot_heatmap(\n",
        "        df_grid,\n",
        "        value_col=\"median_M\",\n",
        "        title=\"Capital load proxy multiple across shock probability and tail index\",\n",
        "        out_path=os.path.join(out_dir, \"fig_capital_load_heatmap.png\")\n",
        "    )\n",
        "    plot_heatmap(\n",
        "        df_grid,\n",
        "        value_col=\"exclusion_rate\",\n",
        "        title=\"Exclusion rate across shock probability and tail index with threshold T equals 6\",\n",
        "        out_path=os.path.join(out_dir, \"fig_exclusion_rate_heatmap.png\")\n",
        "    )\n",
        "    plot_exclusion_boundary_curve(seed=0, out_dir=out_dir, regime=\"C\", xi=1.3, backstop_share=0.0, T=6.0)\n",
        "\n",
        "    # Appendix equations\n",
        "    write_appendix_equations_tex(out_dir=out_dir)\n",
        "\n",
        "    # Mermaid files\n",
        "    write_mermaid_files(out_dir=out_dir)\n",
        "\n",
        "    # Save run metadata for reproducibility\n",
        "    meta = {\n",
        "        \"seed\": 0,\n",
        "        \"outputs_dir\": out_dir,\n",
        "        \"notes\": \"Synthetic simulator for governance stress testing. See appendix_equations.tex for model.\"\n",
        "    }\n",
        "    with open(os.path.join(out_dir, \"run_metadata.json\"), \"w\", encoding=\"utf8\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "    print(\"Done. Outputs written to:\", out_dir)\n",
        "    print(\"Main text figures: fig_capital_load_heatmap.png, fig_exclusion_rate_heatmap.png, fig_exclusion_boundary_curve.png\")\n",
        "    print(\"Appendix equations: appendix_equations.tex\")\n",
        "    print(\"Mermaid diagrams: mermaid_fig_*.mmd\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPosguMhTpMN",
        "outputId": "5826caab-38f9-4d93-c067-e39097405fe3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Outputs written to: outputs\n",
            "Main text figures: fig_capital_load_heatmap.png, fig_exclusion_rate_heatmap.png, fig_exclusion_boundary_curve.png\n",
            "Appendix equations: appendix_equations.tex\n",
            "Mermaid diagrams: mermaid_fig_*.mmd\n"
          ]
        }
      ]
    }
  ]
}